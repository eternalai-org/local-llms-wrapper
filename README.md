# Local LLMs Toolkit

[![License: MIT](https://img.shields.io/badge/License-MIT-blue.svg)](LICENSE)

> Deploy and manage large language models locally with minimal setup.

## ðŸ“‹ Table of Contents

- [Overview](#overview)
- [Installation](#installation)
- [Usage](#usage)
- [Contributing](#contributing)

## ðŸ”­ Overview

The **Local LLMs Toolkit** empowers developers to deploy state-of-the-art large language models directly on their machines. Bypass cloud services, maintain privacy, and reduce costs while leveraging powerful AI capabilities.

## âœ¨ Features

- **Simple Deployment**: Get models running with minimal configuration
- **Filecoin Integration**: Upload and download models directly from decentralized storage (Filecoin)

## ðŸ†• Latest Updates

Version 2.0.8 includes:
- Improved model restart reliability
- Enhanced idle model detection and management
- Better support for Gemma models
- OpenAI-compatible API endpoints
- Fixed memory usage monitoring
- Performance optimizations
- New `memory` command to check model resource usage
- Optimized download process with better concurrency and error handling
- Improved memory diagnostics with detailed system and process information
- Exponential backoff for more reliable downloads
- Better temp file handling to prevent corruption during downloads

## ðŸ“¦ Installation

### MacOS

```bash
bash mac.sh
```

### Verification

Confirm successful installation:

```bash
source local_llms/bin/activate
local-llms version
```

## ðŸš€ Usage

### Managing Models

```bash
# Check if model is available locally
local-llms check --hash <filecoin_hash>

# Upload a model to Filecoin
local-llms upload --folder-name <folder_name> --task <task>

# Download a model from Filecoin
local-llms download --hash <filecoin_hash>

# Start a model
local-llms start --hash <filecoin_hash>

# Example
local-llms start --hash bafkreiecx5ojce2tceibd74e2koniii3iweavknfnjdfqs6ows2ikoow6m

# Check running models
local-llms status

# Stop the current model
local-llms stop

# Check memory usage of running model
local-llms memory

# Get detailed memory usage in JSON format
local-llms memory --json
```
### Important Notes on Uploading Models

When using the `upload` command, the following flags are required:

- **`--folder-name`**: Specifies the directory containing your model. The model file must have the same name as this folder.
- **`--task`**: Defines the model's primary purpose (e.g., `text-generation`, `text-classification`, `image-recognition`, etc.)

**Example Upload Command:**

```bash
# Upload a GPT model for text generation
local-llms upload --folder-name llama2-7b --task text-generation

# Upload a BERT model for text classification
local-llms upload --folder-name bert-classifier --task text-classification
```

Make sure your folder structure follows this convention:
```
llama2-7b/
â”œâ”€â”€ llama2-7b (the model file with `gguf` format)
```


## ðŸ‘¥ Contributing

Contributions welcome! Please see [CONTRIBUTING.md](CONTRIBUTING.md) for details.